import numpy as np
import os
import pickle
import random


def create_data_split(X, Y, m, data_split):
    """
    Creates a train/val/test split given a list of sizes for each split.
    :param X: (np.array)
    :param Y: (np.array)
    :param m: (int) Number of datapoints
    :param data_split: (list) containing the % of each split in the order
        [size_train, size_val, size_test]
    :return: a tuple of size three containing the (X_split, Y_split) arrays
        for each type of split
    """
    # Set seed and verify data_split is appropriate given dataset size
    random.seed(42)
    assert sum(data_split) == 1

    # Get sizes for each split
    train_size = int(m * data_split[0])
    val_size = int(m * data_split[1])
    test_size = m - train_size - val_size

    # Create permutation and indexes for each split
    perm = np.random.permutation(m)
    train_id, val_id, test_id = perm[: train_size], \
                                perm[train_size: train_size + val_size], \
                                perm[train_size + val_size:]

    # Generate datasets
    X_train, Y_train = np.take(X, train_id, axis=0), \
                       np.take(Y, train_id).reshape(train_size, 1)
    X_val, Y_val = np.take(X, val_id, axis=0), \
                   np.take(Y, val_id).reshape(val_size, 1)
    X_test, Y_test = np.take(X, test_id, axis=0), \
                     np.take(Y, test_id).reshape(test_size, 1)

    split_data = (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)
    return split_data


def process_sat_data(base_data_file, data_dir, output_variable, data_split):
    """
    Pre-processes Satellite Data and creates train/val/test splits.
    :param base_data_file: (str) Path to the file generated by the GGEarth
        script
    :param data_dir: (str) Path to where train, val, test files will be
        exported
    :param output_variable: (str) Selected output variable
    :param data_split: (list) containing the % of each split in the order
        [size_train, size_val, size_test]
    :return: void
    """
    # Load file
    try:
        data = pickle.load(open(base_data_file, 'rb'))
    except FileNotFoundError:
        print('[ERROR] Dataset not found.')

    # Filter for selected output variable
    data = data[data['type'] == output_variable]

    # Get dataset size
    m = len(data)

    # Get features and labels
    X = np.array(data['imagery'].to_list())

    # Ensure dimension ordering is in line with PyTorch
    if X.shape[1] > X.shape[3]:
        X = np.moveaxis(X, source=3, destination=1)

    # Distinguish between preprocessing for classification and regression
    if "AQI" in output_variable:
        Y = data['AQI_level'].to_numpy().reshape(m, 1)
    else:
        Y = data['value'].to_numpy().reshape(m, 1)

    # Create train/test/split
    split_data = create_data_split(X, Y, m, data_split)

    # Create directories for each split and save each data point separately
    # for efficient access
    for i, split in enumerate(['train', 'val', 'test']):
        # Make directory
        path = os.path.join(data_dir, '{}_{}'.format(output_variable, split))
        if not os.path.exists(path):
            os.mkdir(path)

        # Get data
        X_split, Y_split = split_data[i][0], split_data[i][1]

        # Save each data point
        for j in range(Y_split.shape[0]):
            x, y = X_split[j], Y_split[j]
            example_path = os.path.join(path, '{:05d}.npz'.format(j))
            np.savez_compressed(example_path, X=x, y=y)
