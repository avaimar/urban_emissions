import pandas as pd

# Set up selected year, months, days, hour
years = ["2020"]
months = [str(num).zfill(2) for num in list(range(1, 13))]
all_days = [str(num).zfill(2) for num in list(range(1, 32))]
hours = [str(num).zfill(2) for num in list(range(24))]

# Base URL
base_url = "https://s3-us-west-1.amazonaws.com//files.airnowtech.org/airnow"

# Set up data containers
hourly_data = pd.DataFrame()

# Loop over all years, months, days, hours
for year in years:
    print('[INFO] Loading data for the year {}'.format(year))
    tab_year = pd.DataFrame()
    for month in months:
        print('[INFO] Loading data for month {}, year {}'.format(month, year))
        # Pick the correct number of days for the month
        if month in ["04", "06", "09", "11"]:
            days = all_days[:-1]
        elif month == "02":
            days = all_days[:-3]
        else:
            days = all_days
        # Create a new month table
        tab_month = pd.DataFrame()
        for day in days:
            print('[INFO] Loading data for day {}, month {}, year {}'.format(
                day, month, year))
            # Create a day table
            tab_day = pd.DataFrame()
            for hour in hours:
                # Get HourlyData query
                query = "{}/{}/{}{}{}/HourlyAQObs_{}{}{}{}.dat".format(
                    base_url,
                    year,
                    year, month, day,
                    year, month, day, hour)

                # Read data and append to day table
                try:
                    tab_hour = pd.read_csv(query)
                    tab_day = pd.concat([tab_day, tab_hour], axis=0)

                except:
                    print("[INFO] Error loading dataset {}/{}/{}/{}".format(
                        year, month, day, hour))

            # Confirm measurements
            if len(tab_day['PM25_Unit'].unique()) > 2:
                print("[WARNING] PM25 Units: Expected UG/M3",
                      tab_day['PM25_Unit'].unique())

            if len(tab_day['OZONE_Unit'].unique()) > 2:
                print("[WARNING] OZONE Units: Expected PPB, ",
                      tab_day['OZONE_Unit'].unique())

            if len(tab_day['PM10_Unit'].unique()) > 2:
                print("[WARNING] PM10 Units: Expected UG/M3, ",
                      tab_day['PM10_Unit'].unique())

            if len(tab_day['CO_Unit'].unique()) > 2:
                print("[WARNING] CO Units: Expected PPM, ",
                      tab_day['CO_Unit'].unique())

            if len(tab_day['NO2_Unit'].unique()) > 2:
                print("[WARNING] NO2 Units: Expected PPB, ",
                      tab_day['NO2_Unit'].unique())

            if len(tab_day['SO2_Unit'].unique()) > 2:
                print("[WARNING] SO2 Units: Expected PPB, ",
                      tab_day['SO2_Unit'].unique())

            # Correct missing AQI values (-999)
            tab_hour = tab_hour[tab_hour['PM10_AQI'] != -999]
            tab_hour = tab_hour[tab_hour['PM25_AQI'] != -999]
            tab_hour = tab_hour[tab_hour['OZONE_AQI'] != -999]
            tab_hour = tab_hour[tab_hour['NO2_AQI'] != -999]

            # Compute daily average and append to month table
            tab_day = tab_day.drop(
                columns=['ValidTime', 'ValidDate', 'EPARegion', 'Elevation',
                         'GMTOffset', 'CountryCode', 'StateName', 'DataSource',
                         'ReportingArea_PipeDelimited', 'OZONE_Measured',
                         'PM10_Measured', 'PM25_Measured',  'NO2_Measured',
                         'OZONE_Unit', 'PM25_Unit', 'NO2_Unit', 'PM10_Unit',
                         'CO_Unit', 'SO2_Unit'],
                axis=1)
            tab_day = pd.melt(tab_day,
                              id_vars=['AQSID', 'SiteName', 'Status',
                                       'Latitude', 'Longitude'],
                              var_name='measurement',
                              value_name='value')
            tab_day = tab_day \
                .groupby(['AQSID', 'SiteName', 'Status', 'Latitude',
                          'Longitude', 'measurement'],
                         as_index=False) \
                .mean()
            tab_month = pd.concat([tab_month, tab_day], axis=0)
        # Compute monthly average and append to month table
        tab_month = tab_month \
            .groupby(['AQSID', 'SiteName', 'Status', 'Latitude',
                      'Longitude', 'measurement'], as_index=False) \
            .mean()
        tab_year = pd.concat([tab_year, tab_month], axis=0)

    # Compute annual average and append to hourly_data table
    tab_year = tab_year \
        .groupby(['AQSID', 'SiteName', 'Status', 'Latitude', 'Longitude',
                  'measurement'], as_index=False) \
        .mean()
    tab_year['year'] = year
    hourly_data = pd.concat([hourly_data, tab_year], axis=0)

print('[INFO] Data scraping completed.')
hourly_data.to_csv(
    '../01_Data/01_Carbon_emissions/AirNow/World_sites_2020_avg.csv',
    index=False)
